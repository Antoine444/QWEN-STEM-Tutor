{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd8871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn.functional import log_softmax\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "MODEL_PATH  = \"antoine-444/m3_sft_2e-6_model\"\n",
    "DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE  = 8\n",
    "SPLITS      = \"test\"\n",
    "OUT_DIR     = \"/home/eval_out\"\n",
    "\n",
    "TASK_REGISTRY = {\n",
    "    \"MMLU-STEM\" : \"antoine-444/mmlu_stem_dataset\",\n",
    "    \"SciQ\"      : \"antoine-444/sciq_dataset\",\n",
    "    \"AQuA-RAT\"  : \"antoine-444/aqua_rat_dataset\",\n",
    "    \"MedMCQA\"   : \"antoine-444/medmcqa_dataset\",\n",
    "    \"AI2-ARC\"   : \"antoine-444/ai2_arc_dataset\",\n",
    "}\n",
    "\n",
    "LETTER_INDICES = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "# List of STEM subjects in the MMLU STEM split\n",
    "STEM_SUBJECTS = [\n",
    "    \"abstract_algebra\", \"anatomy\", \"astronomy\", \"biology\", \"clinical_knowledge\",\n",
    "    \"college_biology\", \"college_chemistry\", \"college_computer_science\", \"college_mathematics\",\n",
    "    \"college_medicine\", \"college_physics\", \"computer_security\", \"conceptual_physics\",\n",
    "    \"electrical_engineering\", \"elementary_mathematics\", \"formal_logic\", \"high_school_biology\",\n",
    "    \"high_school_chemistry\", \"high_school_computer_science\", \"high_school_mathematics\",\n",
    "    \"high_school_physics\", \"high_school_statistics\", \"logical_fallacies\", \"machine_learning\",\n",
    "    \"medical_genetics\", \"nutrition\", \"professional_medicine\", \"virology\"\n",
    "]\n",
    "\n",
    "# Load only the STEM split once\n",
    "ds = load_dataset(\"antoine-444/mmlu_stem_dataset\", split=SPLITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84540871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcqa_prompt(ex, topic=\"knowledge and skills in advanced master-level STEM courses\"):\n",
    "    \"\"\"\"\n",
    "    Generate a multiple-choice question prompt for the given example.\n",
    "    Args:\n",
    "        ex (dict): Example from the dataset containing 'question', 'choices', and 'answer'.\n",
    "        topic (str): Topic of the questions, default is \"knowledge and skills in advanced master-level STEM courses\".\n",
    "    Returns:\n",
    "        str: Formatted prompt string for the multiple-choice question.\n",
    "    \"\"\"\n",
    "    p  = f\"The following are multiple choice questions (with answers) about {topic}.\\n\\n\"\n",
    "    p += ex[\"question\"] + \"\\n\"\n",
    "    p += \"\".join(f\"{l}. {c}\\n\" for l, c in zip(LETTER_INDICES, ex[\"choices\"]))\n",
    "    p += \"Answer:\"\n",
    "    return p\n",
    "\n",
    "def gold_index(ex):\n",
    "    \"\"\"\"\n",
    "    Get the 0-based index of the correct answer in the choices.\n",
    "    Args:\n",
    "        ex (dict): Example from the dataset containing 'answer'.\n",
    "    Returns:\n",
    "        int: Index of the correct answer in the LETTER_INDICES list.\n",
    "    \"\"\"\n",
    "    return LETTER_INDICES.index(ex[\"answer\"])\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def batched_ll(model, tok, prompts, choice_ids, device):\n",
    "    \"\"\"\"\n",
    "    Compute the log likelihood of each choice in the prompts.\n",
    "    Args:\n",
    "        model (AutoModelForCausalLM): Pretrained language model.\n",
    "        tok (AutoTokenizer): Tokenizer for encoding prompts.\n",
    "        prompts (list of str): List of prompts to evaluate.\n",
    "        choice_ids (torch.Tensor): Tensor containing token IDs for choices.\n",
    "        device (str): Device to run the model on ('cpu' or 'cuda').\n",
    "    Returns:\n",
    "        torch.Tensor: Log likelihoods of each choice in the prompts.\n",
    "    \"\"\"\n",
    "    enc = tok(prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "    logits = model(**enc).logits[:, -1]\n",
    "\n",
    "    return log_softmax(logits, -1)[:, choice_ids]\n",
    "\n",
    "def evaluate_ds(model, tok, ds, prompt_fn, batch_size, device):\n",
    "    \"\"\"\"\n",
    "    Evaluate the dataset using the provided model and tokenizer.\n",
    "    Args:\n",
    "        model (AutoModelForCausalLM): Pretrained language model.\n",
    "        tok (AutoTokenizer): Tokenizer for encoding prompts.\n",
    "        ds (Dataset): Dataset to evaluate.\n",
    "        prompt_fn (function): Function to generate prompts from dataset examples.\n",
    "        batch_size (int): Batch size for evaluation.\n",
    "        device (str): Device to run the model on ('cpu' or 'cuda').\n",
    "    Returns:\n",
    "        tuple: Two numpy arrays containing gold indices and predicted indices.\n",
    "    \"\"\"\n",
    "    # Precompute token IDs for \" A\", \" B\", etc.\n",
    "    choice_ids = torch.tensor(\n",
    "        [tok.encode(f\" {l}\", add_special_tokens=False)[0] for l in LETTER_INDICES],\n",
    "        device=device\n",
    "    )\n",
    "    gold_list, pred_list = [], []\n",
    "\n",
    "    # Iterate by index and build proper example lists\n",
    "    for i in range(0, len(ds), batch_size):\n",
    "        end = min(i + batch_size, len(ds))\n",
    "        examples = [ds[j] for j in range(i, end)]\n",
    "        prompts = [prompt_fn(ex) for ex in examples]\n",
    "        ll = batched_ll(model, tok, prompts, choice_ids, device)\n",
    "        pred_list.extend(ll.argmax(-1).tolist())\n",
    "        gold_list.extend(gold_index(ex) for ex in examples)\n",
    "        \n",
    "    return np.array(gold_list), np.array(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1dc6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”„ Loading model â€¦\")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_PATH, padding_side=\"left\")\n",
    "tok.pad_token_id = tok.eos_token_id\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbaa277",
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "\n",
    "for subj in STEM_SUBJECTS:\n",
    "    sub_ds = ds.filter(lambda ex: ex[\"subject\"] == subj)\n",
    "    if len(sub_ds) == 0:\n",
    "        cms.append(None)\n",
    "        continue\n",
    "    g, p = evaluate_ds(model, tok, sub_ds, mcqa_prompt, BATCH_SIZE, DEVICE)\n",
    "    cm = confusion_matrix(g, p, labels=list(range(len(LETTER_INDICES))))\n",
    "    cms.append(cm)\n",
    "\n",
    "# 1) Build a DataFrame of shape (28 Ã— 16)\n",
    "pairs = [f\"{t}â†’{p}\" for t in LETTER_INDICES for p in LETTER_INDICES]\n",
    "data = []\n",
    "for cm in cms:\n",
    "    if cm is None:\n",
    "        data.append([0]*16)\n",
    "    else:\n",
    "        data.append(cm.flatten().tolist())\n",
    "\n",
    "df = pd.DataFrame(data, index=STEM_SUBJECTS, columns=pairs)\n",
    "\n",
    "# 2) (Optional) Normalize each row so they sum to 1\n",
    "df_norm = df.div(df.sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "import os\n",
    "\n",
    "out_path = \"eval_out\"\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.heatmap(\n",
    "    df_norm, \n",
    "    cmap=\"YlGnBu\", \n",
    "    cbar_kws={\"label\": \"Proportion\"}, \n",
    "    xticklabels=pairs, \n",
    "    yticklabels=[s.replace(\"_\",\" \").title() for s in STEM_SUBJECTS]\n",
    ")\n",
    "plt.xticks(rotation=90, ha=\"center\")\n",
    "plt.title(\"Normalized Trueâ†’Pred Confusion by STEM Subject\")\n",
    "plt.xlabel(\"True â†’ Predicted\")\n",
    "plt.ylabel(\"Subject\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# --- SAVE TO FILE ---\n",
    "save_file = os.path.join(out_path, \"combined_true_pred_confusion_heatmap.png\")\n",
    "plt.savefig(save_file, dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"Saved heatmap to {save_file}\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
